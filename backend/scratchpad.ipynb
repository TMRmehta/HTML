{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6dabe65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys, io\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4547c993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from PyPDF2 import PdfReader\n",
    "client = storage.Client(project=os.getenv(\"GCP_PROJECT_ID\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "982c82ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = client.bucket(bucket_name=\"oncosight-pdf-storage\")\n",
    "blob = bucket.blob(\"GEP_model_SCLCpdf.pdf\")\n",
    "pdf_bytes = blob.download_as_bytes()\n",
    "pdf_stream = io.BytesIO(pdf_bytes)\n",
    "reader = PdfReader(pdf_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5d2e598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESEARCH ARTICLE\n",
      "A Highly Efficient Gene Expression\n",
      "Programming (GEP) Model for AuxiliaryDiagnosis of Small Cell Lung Cancer\n",
      "Zhuang Yu1☯‡*, Haijiao Lu1☯‡, Hongzong Si2, Shihai Liu3, Xianchao Li4, Caihong Gao1,\n",
      "Lianhua Cui5, Chuan Li6, Xue Yang1, Xiaojun Yao7\n",
      "1The Affiliated Hospital of Qingdao University, Department of Oncology, Qingdao, Shandong, P.R. China,\n",
      "2Institute for Computational Science and Engineering, Laboratory of New Fibrous Materials and Modern\n",
      "Textile, the Growing Base for State Key Laboratory, Department of Pharmacy, Qingdao University, Qingdao,Shandong, P.R. China, 3The Affiliated Hospital of Qingdao University, The Central Laboratory, Qingdao,\n",
      "Shandong, P.R. China, 4Department of Pharmacy, Qingdao University, Qingdao, Shandong, P.R. China,\n",
      "5Department of Public Health, Qingdao University Medical College, Qingdao, Shandong, P.R. China, 6The\n",
      "Affiliated Hospital of Qingdao University, Department of Thoracic Surgery, Qingdao, Shandong, P.R. China,\n",
      "7Department of Chemistry, Lanzhou University, Lanzhou, Gansu, P.R. China\n",
      "☯These authors contributed equally to this work.\n",
      "‡These authors are co-first authors on this work.\n",
      "*yuzhuang2002@163.com\n",
      "Abstract\n",
      "Background\n",
      "Lung cancer is an important and common cancer that constitutes a major public health prob-\n",
      "lem, but early detection of small cell lung cancer can significantly improve the survival rateof cancer patients. A number of serum biomarkers have been used in the diagnosis of lungcancers; however, they exhibit low sensitivity and specificity.\n",
      "Methods\n",
      "We used biochemical methods to measure blood levels of lactate dehydrogenase (LDH), C-reactive protein (CRP), Na\n",
      "+,C l-, carcino-embryonic antigen (CEA), and neuron specific\n",
      "enolase (NSE) in 145 small cell lung cancer (SCLC) patients and 155 non-small cell lungcancer and 155 normal controls. A gene expression programming (GEP) model and Re-ceiver Operating Characteristic (ROC) curves incorporating these biomarkers was devel-oped for the auxiliary diagnosis of SCLC.\n",
      "Results\n",
      "After appropriate modification of the parameters, the GEP model was initially set up basedon a training set of 115 SCLC patients and 125 normal controls for GEP model generation.Then the GEP was applied to the remaining 60 subjects (the test set) for model validation.GEP successfully discriminated 281 out of 300 cases, showing a correct classification ratefor lung cancer patients of 93.75% (225/240) and 93.33% (56/60) for the training and testsets, respectively. Another GEP model incorporating four biomarkers, including CEA, NSE,\n",
      "PLOS ONE | DOI:10.1371/journal.pone.0125517 May 21, 2015 1/1 9\n",
      "OPEN ACCESS\n",
      "Citation: Yu Z, Lu H, Si H, Liu S, Li X, Gao C, et al.\n",
      "(2015) A Highly Efficient Gene ExpressionProgramming (GEP) Model for Auxiliary Diagnosis ofSmall Cell Lung Cancer. PLoS ONE 10(5):e0125517. doi:10.1371/journal.pone.0125517\n",
      "Academic Editor: Lanjing Zhang, University Medical\n",
      "Center of Princeton/Rutgers Robert Wood JohnsonMedical School, UNITED STATES\n",
      "Received: May 20, 2014\n",
      "Accepted: March 24, 2015\n",
      "Published: May 21, 2015\n",
      "Copyright: © 2015 Yu et al. This is an open access\n",
      "article distributed under the terms of the Creative\n",
      "Commons Attribution License , which permits\n",
      "unrestricted use, distribution, and reproduction in anymedium, provided the original author and source arecredited.\n",
      "Data Availability Statement: All relevant data are\n",
      "within the paper and its Supporting Information files.\n",
      "Funding: This work was supported by Jieping Wu\n",
      "foundation: 320.6750.13210 and Jieping Wufoundation: 320.6753.1219. The funders had no rolein study design, data collection and analysis, decisionto publish, or preparation of the manuscript.\n",
      "Competing Interests: The authors have declared\n",
      "that no competing interests exist.\n"
     ]
    }
   ],
   "source": [
    "for i, page in enumerate(reader.pages):\n",
    "    print(page.extract_text())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14718005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dd9140f",
   "metadata": {},
   "source": [
    "# Graph Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "615e3102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, json\n",
    "from typing import TypedDict, Annotated, List, Dict, Any, Optional, Generator, Literal\n",
    "from datetime import datetime\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage\n",
    "from langchain.tools import BaseTool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# backend_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "# sys.path.insert(0, backend_dir)\n",
    "\n",
    "from AgentTools.PLOS import PLOSPDFDownload, PLOSSearchTool\n",
    "from AgentTools.Reddit import RedditSearch\n",
    "from AgentTools.Summarizer import summarize_pdf, list_pdfs\n",
    "\n",
    "# Load environment variables\n",
    "# dotenv_path = os.path.join(os.path.dirname(__file__), '..', '.env')\n",
    "# load_dotenv(dotenv_path)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model='gemini-2.5-flash',\n",
    "    temperature=0.3,\n",
    "    api_key=os.getenv('GOOGLE_API_KEY'),\n",
    "    max_tokens=1024*4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "238d8b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state structure\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    current_query: str\n",
    "    search_results: Dict[str, Any]\n",
    "    tool_calls_history: List[Dict[str, Any]]\n",
    "    reasoning_steps: List[str]\n",
    "    sources_used: List[Dict[str, str]]\n",
    "    final_answer: Optional[str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b78aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tools\n",
    "plos_search = PLOSSearchTool()\n",
    "plos_downloader = PLOSPDFDownload()\n",
    "reddit_search = RedditSearch()\n",
    "\n",
    "# Create tool list\n",
    "tools = [plos_search, plos_downloader, reddit_search, summarize_pdf, list_pdfs]\n",
    "\n",
    "# Bind tools to LLM\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4e05410",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a compassionate and knowledgeable Cancer Research Assistant designed to help patients and their families understand cancer-related research and experiences.\n",
    "\n",
    "CORE PRINCIPLES:\n",
    "1. EVIDENCE-BASED: Every claim must be grounded in either PLOS research papers or Reddit community posts\n",
    "2. NO HALLUCINATION: Never generate medical advice or facts not directly from sources\n",
    "3. COMPASSIONATE: Communicate with empathy, understanding you're speaking to patients/families\n",
    "4. ACCESSIBLE: Explain complex medical concepts in simple, understandable terms\n",
    "5. TRANSPARENT: Always cite your sources and acknowledge limitations or conflicting evidence\n",
    "\n",
    "REASONING APPROACH (ReAct/Chain of Thought):\n",
    "Before answering any query, follow these steps:\n",
    "\n",
    "1. UNDERSTAND: What is the user really asking? Break down complex questions.\n",
    "2. PLAN: What information do I need? Which tools should I use?\n",
    "3. SEARCH: Use tools strategically:\n",
    "   - PLOS for scientific research (max 5 keywords)\n",
    "   - Reddit for patient experiences (max 8 keywords)\n",
    "   - Can search multiple times with different keywords if needed\n",
    "4. EVALUATE: Assess the quality and relevance of found information\n",
    "5. SYNTHESIZE: Combine findings, noting agreements and conflicts\n",
    "6. COMMUNICATE: Present findings clearly with proper citations\n",
    "\n",
    "TOOL USAGE GUIDELINES:\n",
    "- PLOSSearch: Use for scientific papers, treatments, clinical studies\n",
    "- RedditSearch: Use for patient experiences, side effects, quality of life\n",
    "- PLOSPDFDownload: Use when detailed analysis of specific papers is needed\n",
    "- summarize_pdf: Use to extract key findings from downloaded papers\n",
    "- list_pdfs: Use to get a list of all PDF research papers stored in GCS bucket\n",
    "\n",
    "IMPORTANT RULES:\n",
    "- If sources conflict, explain both perspectives\n",
    "- Do not use more than 10 tool calls for generating a single answer. If you need more info from the user, as follow up questions at the end of each answer.\n",
    "- If no relevant sources found, clearly state this limitation\n",
    "- Never provide medical advice - only share what research/experiences say\n",
    "- Always encourage consulting healthcare providers for personal medical decisions\n",
    "- Use simple analogies to explain complex medical terms\n",
    "\n",
    "Remember: You're helping vulnerable people understand difficult information. Be accurate, honest, compassionate and kind.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b064964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node functions\n",
    "def reasoning_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Initial reasoning about the query\"\"\"\n",
    "    current_query = state['messages'][-1].content if state['messages'] else \"\"\n",
    "    \n",
    "    reasoning_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", SYSTEM_PROMPT),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\"human\", \"\"\"Based on the current query: '{query}'\n",
    "        \n",
    "        Think step by step:\n",
    "        1. What specific information is the user seeking?\n",
    "        2. What sources would be most appropriate (research papers vs patient experiences)?\n",
    "        3. What keywords would be most effective for searching (remember: max 5 for PLOS, max 8 for Reddit)?\n",
    "        4. Should I search multiple times with different keywords?\n",
    "        \n",
    "        Provide your reasoning and initial search plan.\"\"\")\n",
    "    ])\n",
    "    \n",
    "    chain = reasoning_prompt | llm\n",
    "    response = chain.invoke({\n",
    "        \"messages\": state['messages'],\n",
    "        \"query\": current_query\n",
    "    })\n",
    "    \n",
    "    reasoning_steps = [response.content]\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"current_query\": current_query,\n",
    "        \"reasoning_steps\": reasoning_steps\n",
    "    }\n",
    "\n",
    "# def tool_calling_node(state: AgentState) -> AgentState:\n",
    "#     \"\"\"Decide which tools to call based on reasoning\"\"\"\n",
    "    \n",
    "#     tool_prompt = ChatPromptTemplate.from_messages([\n",
    "#         (\"system\", SYSTEM_PROMPT),\n",
    "#         MessagesPlaceholder(variable_name=\"messages\"),\n",
    "#         (\"human\", \"\"\"Based on your reasoning: {reasoning}\n",
    "        \n",
    "#         Current search results so far: {search_results}\n",
    "        \n",
    "#         Decide which tools to call next. Remember:\n",
    "#         - You can call the same tool multiple times with different keywords\n",
    "#         - Start with broad searches, then narrow down if needed\n",
    "#         - Use PLOS for scientific evidence, Reddit for patient experiences\n",
    "#         - Download and summarize PDFs only for highly relevant papers\n",
    "        \n",
    "#         Call the appropriate tools now.\"\"\")\n",
    "#     ])\n",
    "    \n",
    "#     chain = tool_prompt | llm_with_tools\n",
    "    \n",
    "#     response = chain.invoke({\n",
    "#         \"messages\": state['messages'],\n",
    "#         \"reasoning\": state.get('reasoning_steps', []),\n",
    "#         \"search_results\": json.dumps(state.get('search_results', {}), indent=2)\n",
    "#     })\n",
    "    \n",
    "#     # Update messages with tool calls\n",
    "#     messages = state['messages'] + [response]\n",
    "    \n",
    "#     return {\n",
    "#         **state,\n",
    "#         \"messages\": messages\n",
    "#     }\n",
    "\n",
    "def tool_calling_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Decide which tool to call (one at a time for Gemini)\"\"\"\n",
    "    \n",
    "    # Check how many tool calls we've already made\n",
    "    tool_calls_count = len(state.get('tool_calls_history', []))\n",
    "    \n",
    "    tool_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", SYSTEM_PROMPT + \"\"\"\n",
    "IMPORTANT: You can only call ONE tool at a time. Choose the most important tool call for this step.\n",
    "If you need multiple searches, you'll get another chance in the next iteration.\"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\"human\", \"\"\"Based on your reasoning: {reasoning}\n",
    "        \n",
    "Current search results so far: {search_results}\n",
    "Tool calls made so far: {tool_calls_count}\n",
    "        \n",
    "Choose ONE tool to call next. Priority:\n",
    "1. If no research found yet: Use PLOS search first\n",
    "2. If research exists but no patient experiences: Use Reddit search  \n",
    "3. If you have both but need specific paper details: Use PDF download\n",
    "4. If you have content to summarize: Use summarizer\n",
    "\n",
    "Call only ONE tool now.\"\"\")\n",
    "    ])\n",
    "    \n",
    "    chain = tool_prompt | llm_with_tools\n",
    "    \n",
    "    response = chain.invoke({\n",
    "        \"messages\": state['messages'],\n",
    "        \"reasoning\": state.get('reasoning_steps', []),\n",
    "        \"search_results\": json.dumps(state.get('search_results', {}), indent=2),\n",
    "        \"tool_calls_count\": tool_calls_count\n",
    "    })\n",
    "    \n",
    "    # Update messages with tool calls\n",
    "    messages = state['messages'] + [response]\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"messages\": messages\n",
    "    }\n",
    "\n",
    "\n",
    "def process_tools_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Process the results from tool calls\"\"\"\n",
    "    \n",
    "    # Get the last message which should contain tool calls\n",
    "    last_message = state['messages'][-1]\n",
    "    \n",
    "    # Execute tools and collect results\n",
    "    tool_node = ToolNode(tools)\n",
    "    result = tool_node.invoke(state)\n",
    "    \n",
    "    # Extract and store tool results\n",
    "    search_results = state.get('search_results', {})\n",
    "    tool_calls_history = state.get('tool_calls_history', [])\n",
    "    \n",
    "    # Parse tool results and update search_results\n",
    "    for message in result['messages']:\n",
    "        if hasattr(message, 'content'):\n",
    "            try:\n",
    "                # Store the raw results\n",
    "                timestamp = datetime.now().isoformat()\n",
    "                search_results[timestamp] = message.content\n",
    "                tool_calls_history.append({\n",
    "                    'timestamp': timestamp,\n",
    "                    'result': message.content\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"messages\": result['messages'],\n",
    "        \"search_results\": search_results,\n",
    "        \"tool_calls_history\": tool_calls_history\n",
    "    }\n",
    "\n",
    "def synthesis_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Synthesize all findings into a coherent answer\"\"\"\n",
    "    \n",
    "    synthesis_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", SYSTEM_PROMPT),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\"human\", \"\"\"You have gathered the following information:\n",
    "\n",
    "Search Results:\n",
    "{search_results}\n",
    "\n",
    "Now synthesize this information to answer the original query: '{query}'\n",
    "\n",
    "Guidelines for your response:\n",
    "1. Start with a brief, empathetic acknowledgment of the question\n",
    "2. Present findings organized by source type (Research papers vs Patient experiences)\n",
    "3. Use simple language and explain medical terms with analogies\n",
    "4. Clearly cite each source (paper title or Reddit post)\n",
    "5. If there are conflicting findings, explain both perspectives\n",
    "6. End with important caveats or encouragement to consult healthcare providers\n",
    "7. If insufficient information was found, be honest about limitations\n",
    "\n",
    "Format your response with clear sections:\n",
    "- Summary of Findings\n",
    "- Research Evidence (from PLOS)\n",
    "- Patient Experiences (from Reddit)\n",
    "- Important Considerations\n",
    "- Sources Used\n",
    "\n",
    "Remember: Be accurate, compassionate, and never generate information not found in sources.\"\"\")\n",
    "    ])\n",
    "    \n",
    "    chain = synthesis_prompt | llm\n",
    "    \n",
    "    response = chain.invoke({\n",
    "        \"messages\": state['messages'],\n",
    "        \"search_results\": json.dumps(state.get('search_results', {}), indent=2),\n",
    "        \"query\": state['current_query']\n",
    "    })\n",
    "    \n",
    "    # Extract sources from search results\n",
    "    sources_used = []\n",
    "    search_results = state.get('search_results', {})\n",
    "    \n",
    "    for timestamp, result in search_results.items():\n",
    "        if isinstance(result, str):\n",
    "            try:\n",
    "                result_data = json.loads(result) if isinstance(result, str) else result\n",
    "                \n",
    "                # Extract PLOS sources\n",
    "                if 'articles' in result_data:\n",
    "                    for article in result_data['articles']:\n",
    "                        sources_used.append({\n",
    "                            'type': 'PLOS Research',\n",
    "                            'title': article.get('title', 'Unknown'),\n",
    "                            'id': article.get('id', '')\n",
    "                        })\n",
    "                \n",
    "                # Extract Reddit sources\n",
    "                elif isinstance(result_data, list) and len(result_data) > 0:\n",
    "                    if 'subreddit' in result_data[0]:\n",
    "                        for post in result_data[:3]:  # Limit to top 3 Reddit posts\n",
    "                            sources_used.append({\n",
    "                                'type': 'Reddit Post',\n",
    "                                'title': post.get('title', 'Unknown'),\n",
    "                                'subreddit': post.get('subreddit', '')\n",
    "                            })\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"messages\": state['messages'] + [AIMessage(content=response.content)],\n",
    "        \"final_answer\": response.content,\n",
    "        \"sources_used\": sources_used\n",
    "    }\n",
    "\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    \"\"\"Decide whether to continue searching or provide final answer\"\"\"\n",
    "    \n",
    "    # Check if we have enough information\n",
    "    search_results = state.get('search_results', {})\n",
    "    tool_calls = state.get('tool_calls_history', [])\n",
    "    \n",
    "    # If we've made at least 2 tool calls and have some results, check if we need more\n",
    "    if len(tool_calls) >= 2 and search_results:\n",
    "        # Ask LLM if we have enough information\n",
    "        check_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are evaluating if you have enough information to answer a query.\"),\n",
    "            (\"human\", \"\"\"Query: {query}\n",
    "            \n",
    "Current search results: {results}\n",
    "\n",
    "Do you have enough relevant information to provide a comprehensive answer? \n",
    "Reply with ONLY 'continue' if you need more searches, or 'synthesize' if you have enough.\"\"\")\n",
    "        ])\n",
    "        \n",
    "        chain = check_prompt | llm\n",
    "        response = chain.invoke({\n",
    "            \"query\": state['current_query'],\n",
    "            \"results\": json.dumps(search_results, indent=2)[:2000]  # Truncate for context\n",
    "        })\n",
    "        \n",
    "        if 'synthesize' in response.content.lower():\n",
    "            return 'synthesize'\n",
    "    \n",
    "    # If we've made too many calls (>5), synthesize what we have\n",
    "    if len(tool_calls) > 10:\n",
    "        return 'synthesize'\n",
    "    \n",
    "    return 'continue'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5b54d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cancer_research_agent():\n",
    "    \"\"\"Create and compile the LangGraph agent\"\"\"\n",
    "    \n",
    "    # Initialize the graph\n",
    "    workflow = StateGraph(AgentState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"reason\", reasoning_node)\n",
    "    workflow.add_node(\"call_tools\", tool_calling_node)\n",
    "    workflow.add_node(\"process_tools\", process_tools_node)\n",
    "    workflow.add_node(\"synthesize\", synthesis_node)\n",
    "    \n",
    "    # Define the flow\n",
    "    workflow.set_entry_point(\"reason\")\n",
    "    \n",
    "    # Add edges\n",
    "    workflow.add_edge(\"reason\", \"call_tools\")\n",
    "    workflow.add_edge(\"call_tools\", \"process_tools\")\n",
    "    \n",
    "    # Conditional edge from process_tools\n",
    "    workflow.add_conditional_edges(\n",
    "        \"process_tools\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"continue\": \"call_tools\",\n",
    "            \"synthesize\": \"synthesize\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # End after synthesis\n",
    "    workflow.add_edge(\"synthesize\", END)\n",
    "    \n",
    "    # Compile the graph\n",
    "    app = workflow.compile()\n",
    "    \n",
    "    return app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8590c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientAgent():\n",
    "\t\"\"\"Main interface for the Cancer Research Assistant\"\"\"\n",
    "\t\n",
    "\tdef __init__(self, history:Optional[List] = None):\n",
    "\t\tself.agent = create_cancer_research_agent()\n",
    "\n",
    "\t\tif history is None:\n",
    "\t\t\tself.conversation_history = []\n",
    "\t\n",
    "\tdef query(self, question: str, mode:Optional[Literal['default', 'stream']] = 'default') -> Dict[str, Any]:\n",
    "\t\t\"\"\"Process a user query and return the response with sources\"\"\"\n",
    "\t\t\n",
    "\t\t# Add user message to history\n",
    "\t\tuser_message = HumanMessage(content=question)\n",
    "\t\tself.conversation_history.append(user_message)\n",
    "\t\t\n",
    "\t\t# Initialize state\n",
    "\t\tinitial_state = {\n",
    "\t\t\t\"messages\": self.conversation_history[-10:],  # Keep last 10 messages for context\n",
    "\t\t\t\"current_query\": question,\n",
    "\t\t\t\"search_results\": {},\n",
    "\t\t\t\"tool_calls_history\": [],\n",
    "\t\t\t\"reasoning_steps\": [],\n",
    "\t\t\t\"sources_used\": [],\n",
    "\t\t\t\"final_answer\": None\n",
    "\t\t}\n",
    "\t\t\n",
    "\t\tresult = self.agent.invoke(initial_state)\n",
    "\t\tprint(result)\n",
    "\t\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9fbb82de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\Software Projects\\Oncosight.AI\\backend\\.venv\\Lib\\site-packages\\proto\\marshal\\rules\\enums.py:37: UserWarning: Unrecognized FinishReason enum value: 12\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m agent = PatientAgent()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat are the latest advancements in lung cancer research?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mPatientAgent.query\u001b[39m\u001b[34m(self, question, mode)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Initialize state\u001b[39;00m\n\u001b[32m     18\u001b[39m initial_state = {\n\u001b[32m     19\u001b[39m \t\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.conversation_history[-\u001b[32m10\u001b[39m:],  \u001b[38;5;66;03m# Keep last 10 messages for context\u001b[39;00m\n\u001b[32m     20\u001b[39m \t\u001b[33m\"\u001b[39m\u001b[33mcurrent_query\u001b[39m\u001b[33m\"\u001b[39m: question,\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m \t\u001b[33m\"\u001b[39m\u001b[33mfinal_answer\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     26\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Software Projects\\Oncosight.AI\\backend\\.venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:3026\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3023\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3024\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3026\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3027\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3028\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3029\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3030\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3031\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3032\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3033\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3034\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3035\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3036\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3037\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3038\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3039\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3040\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3041\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Software Projects\\Oncosight.AI\\backend\\.venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:2647\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2645\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2646\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2647\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2653\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2654\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2657\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Software Projects\\Oncosight.AI\\backend\\.venv\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:162\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    160\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Software Projects\\Oncosight.AI\\backend\\.venv\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Software Projects\\Oncosight.AI\\backend\\.venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:657\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Software Projects\\Oncosight.AI\\backend\\.venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:401\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    399\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mreasoning_node\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      6\u001b[39m reasoning_prompt = ChatPromptTemplate.from_messages([\n\u001b[32m      7\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, SYSTEM_PROMPT),\n\u001b[32m      8\u001b[39m     MessagesPlaceholder(variable_name=\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m \u001b[33m    Provide your reasoning and initial search plan.\u001b[39m\u001b[33m\"\"\"\u001b[39m)\n\u001b[32m     18\u001b[39m ])\n\u001b[32m     20\u001b[39m chain = reasoning_prompt | llm\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m response = \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_query\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m reasoning_steps = [response.content]\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     29\u001b[39m     **state,\n\u001b[32m     30\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcurrent_query\u001b[39m\u001b[33m\"\u001b[39m: current_query,\n\u001b[32m     31\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreasoning_steps\u001b[39m\u001b[33m\"\u001b[39m: reasoning_steps\n\u001b[32m     32\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Software Projects\\Oncosight.AI\\backend\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3049\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3047\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3048\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3049\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3050\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3051\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Software Projects\\Oncosight.AI\\backend\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:1488\u001b[39m, in \u001b[36mChatGoogleGenerativeAI.invoke\u001b[39m\u001b[34m(self, input, config, code_execution, stop, **kwargs)\u001b[39m\n\u001b[32m   1483\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1484\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1485\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTools are already defined.code_execution tool can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be defined\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1486\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1488\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Software Projects\\Oncosight.AI\\backend\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:383\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    371\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    373\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    378\u001b[39m     **kwargs: Any,\n\u001b[32m    379\u001b[39m ) -> BaseMessage:\n\u001b[32m    380\u001b[39m     config = ensure_config(config)\n\u001b[32m    381\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    382\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    393\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Software Projects\\Oncosight.AI\\backend\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1006\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    997\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    998\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    999\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1003\u001b[39m     **kwargs: Any,\n\u001b[32m   1004\u001b[39m ) -> LLMResult:\n\u001b[32m   1005\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1006\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Software Projects\\Oncosight.AI\\backend\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:825\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    823\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    824\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    831\u001b[39m         )\n\u001b[32m    832\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    833\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Software Projects\\Oncosight.AI\\backend\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1072\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1070\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1076\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Software Projects\\Oncosight.AI\\backend\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:1601\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   1583\u001b[39m request = \u001b[38;5;28mself\u001b[39m._prepare_request(\n\u001b[32m   1584\u001b[39m     messages,\n\u001b[32m   1585\u001b[39m     stop=stop,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1593\u001b[39m     **kwargs,\n\u001b[32m   1594\u001b[39m )\n\u001b[32m   1595\u001b[39m response: GenerateContentResponse = _chat_with_retry(\n\u001b[32m   1596\u001b[39m     request=request,\n\u001b[32m   1597\u001b[39m     **kwargs,\n\u001b[32m   1598\u001b[39m     generation_method=\u001b[38;5;28mself\u001b[39m.client.generate_content,\n\u001b[32m   1599\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m.default_metadata,\n\u001b[32m   1600\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1601\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_response_to_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Software Projects\\Oncosight.AI\\backend\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:782\u001b[39m, in \u001b[36m_response_to_result\u001b[39m\u001b[34m(response, stream, prev_usage)\u001b[39m\n\u001b[32m    780\u001b[39m generation_info = {}\n\u001b[32m    781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m candidate.finish_reason:\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m     generation_info[\u001b[33m\"\u001b[39m\u001b[33mfinish_reason\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mcandidate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinish_reason\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\n\u001b[32m    783\u001b[39m     \u001b[38;5;66;03m# Add model_name in last chunk\u001b[39;00m\n\u001b[32m    784\u001b[39m     generation_info[\u001b[33m\"\u001b[39m\u001b[33mmodel_name\u001b[39m\u001b[33m\"\u001b[39m] = response.model_version\n",
      "\u001b[31mAttributeError\u001b[39m: 'int' object has no attribute 'name'",
      "During task with name 'reason' and id '75b3c7e3-43f1-6761-e004-7249be059313'"
     ]
    }
   ],
   "source": [
    "agent = PatientAgent()\n",
    "agent.query(\"What are the latest advancements in lung cancer research?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f8ad4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81deaa68",
   "metadata": {},
   "source": [
    "# ChatDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3a415ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from dotenv import load_dotenv\n",
    "from google import cloud\n",
    "from datetime import datetime\n",
    "\n",
    "from Agents.models import ChatRequest, ChatResponse\n",
    "from AgentTools.Summarizer import summarize_title\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "testuser = 'putcZPfnudmgXrMMOG0m'\n",
    "USERS = 'users'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030bf670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Debug message\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Debug message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c1be4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OncoMind-AI (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
